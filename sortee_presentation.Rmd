---
title: "ReSearchOps: a principled framework and guide to computational reproducibility"
author: "Aaron Willcox | Elliot Gould"
date: "`r Sys.Date()`"
output:
  revealjs::revealjs_presentation:
    theme: white
    transition: slide
    center: false
    self_contained: false
    background_transition: fade
    reveal_plugins: [notes]
    includes:
      after_body: footer.html

---

## Research Code 

> - Source code generated each year grows by about 20% (L. Hatton & M. van Genuchten, 2019).
> - Sharing policy increase: 15% in 2015 to 75% in 2020 (Culina et al., 2018).
> - Data handling and processing often informally transmitted (Maer-Matei et al., 2019).
> - Lack of formal training for researchers (Koehler Leman et al., 2020).


<div class="notes">
The amount of source code generated for research continues to grow each year by about 20%, and although code-sharing policies are increasingly adopted by journals, rising from 15% of ecology journals in 2015 to 75% in 2020, data-sharing remains high, but code-sharing is rare.
As the demand for reproducible workflows and transparent research practices grows so does a need for industry best practice. Modern computational methods are an essential skill in today's scientific methodology. 
Computational research skills such as data handling and processing are often informally transmitted yet are pre-requisite skills  for early career researchers demanded by job-advertisements.
Without formal training in code and workflow building, code development is typically an ad-hoc process that occurs over the course of a project's life, with little forethought given to structuring a computational workflow for ensuring data preservation and reproducibility.
</div>

## Computational Reproducibility

> The ability to produce equivalent analytical outcomes from the same data set using the same code and software as the original study  (Fidler et al., 2017).

<div class="notes">
Computational Reproducibility refers to the ability to produce equivalent analytical outcomes from the same data set using the same code and software as the original study (Fidler et al., 2017).
Despite well-developed data and software sharing capabilities, it is still challenging to reproduce published results.
Ecology both applied and basic face unique challenges to achieving reproducibility and transparency due to the types of studies, data collection methods, and data types that are prevalent in these disciplines.
</div>

## Challenges in Ecology

<section>
  <table class="reveal">
    <tr>
      <td><b>Challenge</b></td>
      <td><b>Cause or mechanism</b></td>
      <td><b>Examples</b></td>
      <td><b>Consequences</b></td>
      <td><b>Solutions</b></td>
      <td><b>Source</b></td>
    </tr>
    <tr class="fragment">
      <td>Regularly Updated Data</td>
      <td>Requires active data management, continual data entry, data processing and integration and error checking because data are continually changing.</td>
      <td>long-term observational studies, experiments with repeated sampling, use of automatic sensors, ongoing literature mining, iterative near-term forecasting, adaptive management</td>
      <td>Large burden on small teams without rapid and automated protocols. Data analysis prone to errors without QA/QC protocols. Reproducibility difficult to achieve without pipeline workflows.</td>
      <td>version-control, automated testing, continuous integration and analysis</td>
      <td>Yenni et al., (2019)</td>
    </tr>
    <tr class="fragment">
      <td>'Data 'freshness' or the time between data collection and data use.</td>
      <td>Data freshness is difficult to track due to variation in reporting practices. This difficulty is increased when many data sources are combined. Unknown data freshness or stale data may increase uncertainty and decrease accuracy in conclusions reached. </td>
      <td>SDM's where predictor variables do not capture recent environmental changes, such as rapid habitat loss, or where occurrence records do not coincide with period from which predictor variable captured</td>
      <td>Poor model performance, reduced accuracy of predictions in areas of rapid environmental change, increased risk of negative outcomes of conservation decisions</td>
      <td>Good metadata that includes temporal aspects of original data collection.</td>
      <td>Murray et al., (2021)</td>
    </tr>
    <tr class="fragment">
      <td>Integrating and synthesising independently collected data from many sources</td>
      <td>Ecological data often context specific, with many nuances and details in the study-system being poorly documented. Methods section limits are too small to capture full suite of details.</td>
      <td>Complex modelling studies, conservation-decision-making studies, model transfers</td>
      <td>Data are easily misinterpreted, biases unknown, and may pose statistical issues when integrating across multiple dimensions and sources.</td>
      <td>use of FAIR data principles (FAIR: Findable, Accessible, Interoperable and Reusable), use of TRUST principles: Transparency, Responsibility, User focus, Sustainability and Technology, data archiving practices that adheres to these principles.</td>
      <td>Culina et al., (2018)</td>
    </tr>
    <tr class="fragment">
      <td>Manual / hard-copy data entry</td>
      <td>data collected on data sheets in the field or lab. Data structure not enforced by hand-recording, mistakes in data entry.</td>
      <td>Hard-copy, free-hand field-data recording. Experimental protocols and results recorded by hand in lab-notebooks. </td>
      <td>Errors in data entry may result in serious errors in conclusions, especially if systematic bias in recording errors.</td>
      <td>Digital data recording with the use of schemas to enforce required data structure. Automated testing or QA/QC upon data entry.</td>
      <td>Yenni et al., (2019)</td>
    </tr>
    <tr class="fragment">
      <td>Bio-logging and automated sensors</td>
      <td>Ongoing QA/QC and data processing necessary, no standards for archiving data, most data are undiscoverable and inaccessible.</td>
      <td>Camera traps, weather data, geo-location tracking, remote sensing or drone data, bio-logging data</td>
      <td>Burden on researchers wanting to either share or reuse bio-logging data, datasets unable to be merged.</td>
      <td>FAIR, TRUST principles,  standardised  templates and metadata, workflows for producing archive-quality data files/td>
      <td>(Sequeira et al., 2021)</td></tr>
   </table>
</section>

<div class="notes">

And here is a non-exhaustive set of challenges:
Many long-term observational or monitoring studies, near-term forecasting or adaptive management programs or experiments with repeated sampling face the challenge of regularly updated data, which requires active data management, continual data entry, processing, integration and error checking. This places a large burden on small teams without rapid deployment protocols and automated workflows, and reproducibility is extremely difficult to achieve.
Data freshness or the time between data collection and data use is difficult to track due to variation in reporting practices, and unknown data freshness or stale data may increase uncertainty and decrease accuracy in conclusions reached.

Complex ecological models or conservation-decision making programs often integrate and synthesise independently collected data from many different sources and of different types, but a lack of good metadata and data archiving practices means that nuances and details of data-collection and the study system are poorly documented. This risks data being misinterpreted and may pose statistical issues when integrating across sources and dimensions.

Importantly, much ecological data is recorded by hand on data sheets in the field or in lab notebooks, risking errors because of a lack of enforced schemas during in recording, and a lack of QA/QC at the time of data entry.

Conversely, bio-logging and automated sensors such as camera traps,  geo-location tracking, and remote sensing data are becoming increasingly common, however, there is a huge burden on researchers wanting to either share or reuse bio-logging data because of a lack of data-archiving standards and general processing workflows.
</div>


## Have You Reproduced Lately?

> - Archmiller et al., (2020) Found 74 suitable for CR of the 19 obtained 13 were able to mostly or fully reproduce.
 
> - Obels et al., (2020) 62 articles identified, 41 had data available and 37 had analysis scripts Ran scripts for 31 analysis and reproduced main results for 21 articles.

> - Increase Code Sharing.
> - Organization and Documentation and Training.
> - Good Research Practices.

<div class="notes">
Archmiller et al., (2020) explored the level of computational reproducibility in the field of wildlife science. Found 74 suitable for CR of the 19 obtained 13 were able to mostly or fully reproduce. **They concluded by recommending increase of code sharing, organization and documentation and training**. 

Obels et al., (2020) tried to reproduce results from registered psychology reports between 2014 to 2018. 62 articles identified, 41 had data available and 37 had analysis scripts. They could run scripts for 31 analysis and reproduced main results for 21 articles. **They too recommend good research practices**.
</div>

## {data-background="figures/bestpract.jpg"} 

<div class="notes">
What constitutes best practice?
</div>

## Source of irreproducible results

> - Lack of a workflow framework.
> - Missing software dependencies.
> - Excluded data manipulation steps (Leipzig et al., 2020).
> - **Irreproducibility and a lack of transparency can be overcome by borrowing a set of tools and practices from software engineering, called DevOps**

<div class="notes">
A common source of the inability to reproduce data and code is the lack of a **workflow framework**, missing **software dependencies** and **excluded data manipulation** steps (Leipzig et al., 2020).
What approach can we as scientists utilize to achieve transparency and computational reproducibility? How can we modernize our workflows to meet increasing data sharing requirements? How do modern researchers navigate the plethora of tools available? Irreproducibility and a lack of transparency can be overcome by borrowing a set of tools and practices from software engineering, called DevOps
</div>


## DevOps

> - **Version Control**: Historical context of data and code changes.  
> - **Containers**: System environmental configuration. 
> - **Continuous Practices (CI/CD)**: Quality assurance and automation. 
> - **Testing**: Expected constraints at output.

<div class="notes">
DevOps is commonly used to describe a suit of tools such as ***version control, infrastructure as code, virtualization, continuous delivery/deployment & testing**. Unsurprisingly or perhaps surprisingly, scientists already are adopting these tools in their pipelines. Version control is incorporated into Github, virtualization has now taken the form of containers like Docker and CI/CD can be used with Github Actions, Gitlab CI or Travis-CI. That is, take the already existing practices and tools from DevOps and integrate these into a best practice approach using commonly adopted tools already being implemented in scientific community and, treat the scientific workflow as software product.
</div>

## Modern Scientific Research

> - No differences between researchers from computer science (Yasmin AlNoamany & John A. Borghi, 2018).
> - Computational reproducibility best approached by focusing on software as a product (Hocquet & Wieber, 2021).
> - More easily achieve computational reproducibiltiy.
> - "*Product*" is the reproducible outcome built around a scientific workflow. 

<div class="notes">
As modern scientific researchers, the tools and approaches we use when developing analysis code and software, don’t actually differ substantially from software engineering - we just don’t conceptualise the computational aspects of our research as software engineering.
By treating our analysis code and software as a product we can more easily achieve computational reproducibiltiy. *Product* is the reproducible outcome built around a scientific workflow.
</div>

## ResearchOps

The Case for DevOps in Scientific Applications (de Bayser et al., 2015)

> - Aid in computational reproducibility and transparency of their work (Beaulieu-Jones & Greene, 2017; Wittman & Aukema, 2020)
> - Increase scientific productivity (Peikert & Brandmaier, 2019)
> - Collaborate effectively within and between researchers (Díaz et al., 2019) 


<div class="notes">
ResearchOps, or DevOps for Research, as coined by De Bayser et al., applies DevOps tools and workflows into scientific research contexts. 
By translating DevOps for use in research contexts, ResOps provides a principled framework and guide to achieving computational reproducibility and transparency, and modernising scientific workflows utilising free and open-source software.
By providing the tools and procedures for more seamlessly integrated analysis code and software development process, adopting ResOps practices will improve reproducibility, transparency of research, and increase productivity by providing tried-and-tested procedures for effective collaboration between research members and teams. 
</div>

## Worfkflows, Pipelines & Components! Oh My!

> - **Scientific Workflow**: Overall scope of the research project.
> - **Pipeline**: Execution of each process or stages of the scientific workflow.
> - **Components**: Tools and/or software adopted to execute the pipeline to deliver research outcomes.


<div class="notes">
At a high-level, we consider three aspects of ResOps: The project management level or scope, the pipeline and the components. 

At the project management level is the scientific workflow. **Scientific workflows** represent complex design pipelines that capture processing requirements for researchers throughout their investigations that allow the execution of data collection, data flow, computation, analysis in an integrative method to deliver and publish results (Catlin et al., 2019).  

The pipeline is a set of discrete stages in which a series of automated transformations and tests are performed on the raw data. Each stage is identified through the overall scope of the scientific workflow so that each stage has a purpose. 

The components are the tools used in software development to address inconsistencies in code through automated testing and quality assurance measures so that the product maintains a standard before deployment or delivery. This ensures that the pipeline adheres to data management protocols. For researchers, this means adding testing to their tool kits and automating such tasks through continuous integration that can provide rapid feedback when a component fails and maintain a reliable pipeline.
</div>


## ResearchOps Framework {data-transition="zoom-in"}

<div align=center>
<img src="figures/resops_framework.png" style="background-color:transparent; border:0px; box-shadow:none;"></img>
</div>

## Project Scope {data-transition="fade-in fade-out"}

<div align=center>
<img src="figures/resops_1.png" style="background-color:transparent; border:0px; box-shadow:none;"></img>
</div>

<div class="notes">
The project level is essentially an area where we focus on the "plan" or scope of the research outcome.
By scoping out the processes needed, identifying stakeholders and how to collaborate and communicate results then directs us to design the discrete stages of the pipeline and or our chosen scripting language.
</div>

## Pipeline {data-transition="fade-in fade-out"}

<div align=center>
<img src="figures/resops_2.png" style="background-color:transparent; border:0px; box-shadow:none;"></img>
</div>

<div class="notes">
The pipeline is where the discrete stages are processed by implementing the components in such a way as to meet the design requirements outlined in the scope. Continuous integration can be setup to meet automation requirements as needed such as weekly data reports and more importantly testing expected functionality of the code, the version control system gives the advantage of rolling back errors or feature testing before merging into the master branch and finally, a container preserves the library and dependencies at runtime. 
</div>

## Research Outcome {data-transition="fade-in fade-out"}

<div align=center>
<img src="figures/resops_3.png" style="background-color:transparent; border:0px; box-shadow:none;"></img>
</div>

<div class="notes">
Each of the components in the pipeline addresses a reproducible outcome. Containers such as Docker or Reprozip ensure long term preservation of the methods and results by wrapping the entire process, documentation and results into an executable format. This can then be shared alongside the published paper for code review, computational reproducibility, replication and reuse by others. Users can access the data and analysis products, querying the existing analysis in new ways, or with new data, or reusing parts of the pipeline in new studies.

</div>

## ResearchOps Framework {data-transition="fade-in fade-out"}

<div align=center>
<img src="figures/resops_framework.png" style="background-color:transparent; border:0px; box-shadow:none;"></img>
</div>

<div class="notes">
Ideally to have this framework at the onset of a project would be nice however, going through this process can also be beneficial during a project. This can also be adopted by a single researcher or scaled to larger collaborative research projects. 
To conclude, modern research methods need a modern approach and as journals adopt policies for sharing data, and code, researchers can benefit from ResOps best practice to increase computational reproducibility & transparency by project managing the data and code as a software product. 
</div>

## Thank You! 

<span style="color: #808080;"><em><a href="https://twitter.com/aaron_willcox">@aaron_willcox</a></em></span>
<a href="https://twitter.com/aaron_willcox" class="fa fa-twitter"></a>

<span style="color: #808080;"><em><a href="https://twitter.com/Elliot_Gould_">@Elliot_Gould_</a></em></span>
<a href="https://twitter.com/Elliot_Gould_" class="fa fa-twitter"></a>

## {data-background-iframe="ref.html"}
